# API Keys for different services
# These keys are required to use models from the respective providers.
# Replace 'YOUR_PROVIDER_API_KEY_HERE' with your actual API key.
api_keys:
  openai: YOUR_OPENAI_API_KEY_HERE    # For OpenAI models (e.g., GPT-4, gpt-3.5-turbo)
  claude: YOUR_CLAUDE_API_KEY_HERE   # For Anthropic Claude models. The provider name 'anthropic' is often used in configurations.
  gemini: YOUR_GEMINI_API_KEY_HERE   # For Google Gemini models. The provider name 'google_gemini' is often used in configurations.

# Model selection for different roles within the SuperArchitect workflow.
# Each role dictates how a model is used in the architectural planning process.
# Model names used here should be the exact identifiers provided by the respective LLM providers
# (e.g., 'claude-3-opus-20240229', 'gemini-1.5-pro-latest', 'gpt-4-turbo').
# The term 'friendly model name' in documentation refers to these provider-specific strings.
model_roles: # This section was previously named 'models'.

  # decomposition_model:
  #   Purpose: Takes the high-level user query and breaks it down into logical substeps,
  #            each typically defined by a title and a list of specific questions to be answered.
  #            This is the first step in structuring the architectural planning process.
  #   Capabilities: Requires a model capable of understanding complex requests and decomposing
  #                 them into structured, actionable steps. Strong reasoning and instruction-following
  #                 are key.
  #   Configuration: Assign the provider-specific model string directly.
  #                  Choose a powerful model for best results (e.g., Claude Opus, Gemini Advanced/Pro, GPT-4).
  #   Example Value: 'claude-3-opus-20240229'
  decomposition_model: 'claude-3-opus-20240229' # Example: 'gemini-1.5-pro-latest'

  # consultation_models:
  #   Purpose: A list of models consulted sequentially for *each* question within a decomposed substep.
  #            The system uses the *first successful response* obtained as the "best answer" for that question.
  #            This allows for fallback if one model fails or provides an unsatisfactory answer.
  #   Capabilities: Models should be proficient in providing architectural suggestions or solutions
  #                 for specific, targeted questions. A diverse set of models can yield robust results.
  #   Configuration: A list of provider-specific model strings. The order matters as they are tried sequentially.
  #                  You can list multiple models from different providers if their APIs are configured.
  #   Example Value:
  #     - 'gemini-1.5-pro-latest'
  #     - 'claude-3-sonnet-20240229'
  consultation_models:
    - 'gemini-1.5-pro-latest' # Primary choice
    # - 'claude-3-sonnet-20240229' # Secondary/fallback option. Uncomment and add more as needed.

  # analyzer_model:
  #   Purpose: Used by the AnalyzerEngine. After the "best answer" for a substep's questions is collected
  #            (from consultation_models), this model processes these answers (along with context)
  #            to generate a detailed, structured "instructional guide" for that substep.
  #            The output is typically a JSON object organized by predefined categories
  #            (e.g., "Summary/Overview," "Key Considerations," "Implementation Steps").
  #   Capabilities: Requires a model strong in structured data generation (especially JSON output) and
  #                 detailed elaboration based on provided inputs. It must synthesize information
  #                 into predefined categories effectively.
  #   Configuration: Assign the provider-specific model string directly.
  #                  Choose a model known for good structured output and detailed, coherent responses.
  #   Example Value: 'claude-3-opus-20240229'
  analyzer_model: 'claude-3-opus-20240229' # Example: 'gemini-1.5-pro-latest' (ensure good JSON mode support)

  # synthesis_handler:
  #   Purpose: Used by the SynthesisEngine. This model (or model configuration) is primarily
  #            responsible for generating the introductory and concluding sections of the final
  #            Markdown architectural plan. It provides a narrative frame around the detailed
  #            instructional guides (produced by the analyzer_model for each substep).
  #            (This role was often referred to as 'final_architect_model' in older configurations).
  #   Capabilities: Needs good narrative generation capabilities, coherence, and the ability to
  #                 set context effectively. Can be a different model than those used for more
  #                 structured tasks, perhaps optimized for creativity or writing style.
  #   Configuration: An object specifying the 'provider' and the 'model_name'.
  #                  - 'provider': A string identifying the LLM provider (e.g., "openai", "anthropic", "google_gemini").
  #                    The application uses this to select the correct API key from the 'api_keys' section.
  #                    (E.g., "anthropic" typically maps to 'claude' key, "google_gemini" to 'gemini' key).
  #                  - 'model_name': The specific model string from that provider (e.g., 'claude-3-sonnet-20240229').
  #   Example Value:
  #     provider: 'anthropic'
  #     model_name: 'claude-3-sonnet-20240229'
  synthesis_handler:
    provider: 'anthropic' # Ensure this provider string is understood by the application to use the 'claude' API key.
    model_name: 'claude-3-sonnet-20240229' # A good balance of capability and cost for narrative tasks.
    # --- Other examples ---
    # provider: 'openai'
    # model_name: 'gpt-4-turbo'
    #
    # provider: 'google_gemini' # Ensure this provider string is understood to use the 'gemini' API key.
    # model_name: 'gemini-1.0-pro'

  # summarizing_model:
  #   Purpose (Legacy/Future): This role was potentially intended for generating concise summaries of
  #                            intermediate results or the final plan.
  #   Status: NOT ACTIVELY USED in the current primary workflow for generating the final architectural plan.
  #           It can generally be ignored or removed for the standard SuperArchitect process unless you have
  #           a custom workflow that specifically utilizes it.
  #           If you intend to use it, ensure the consuming code exists and is configured for this model.
  #   Capabilities: If used, would require a model proficient in text summarization.
  #   Configuration: Assign the provider-specific model string directly.
  #   Example Value: 'gemini-1.0-pro' (Note: Not currently used in the main workflow)
  summarizing_model: 'gemini-1.0-pro' # This model is not actively used in the main workflow. Consider removing if not needed.